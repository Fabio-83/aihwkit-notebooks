{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjqem3CNzHeP"
      },
      "source": [
        "## Why Analog AI\n",
        "\n",
        "In-memory computing hardware increases the speed and energy-efficiency needed for the next steps in AI. Analog AI delivers radical performance improvements by combining compute and memory in a single device, eliminating the von Neumann bottleneck. \n",
        "Based on von Neumann architecture, conventional computers perform calculations by repeatedly transferring data between the memory and processor. These trips require time and energy, negatively impacting performance. This is known as the von Neumann bottleneck. \n",
        "\n",
        "<img src=\"img/processing-unit-and-computional-memory.png\" style=\"width:30%; height:30%\" caption=\"Processing unit and conventional memory\"/> \n",
        "<img src=\"img/processing-unit-and-conventional-memory.png\" style=\"width:30%; height:30%\"/> \n",
        "\n",
        "By leveraging the physical properties of in-memory computing devices (example: Phase Change Memory or PCM), computation happens at the same place where the data is stored, drastically reducing energy consumption. Because there is no movement of data, tasks can be performed in a fraction of the time and with much less energy. This is different from a conventional computer, where the data is transferred from the DRAM memory to the CPU every time a computation is done. For example, moving 64 bits of data from DRAM to CPU consumes 1-2nJ, which is 10,000–2,000,000 times more energy than is dissipated in a PCM device performing a multiplication operation (1-100fJ). Also, PCM does not consume power when the devices are inactive, and the data will be retained for up to 10 years even when the power supply is turned off. \n",
        "\n",
        "## The physics behind PCM\n",
        "\n",
        " With PCM, when an electrical pulse is applied to the material, it changes the conductance of the device by switching the material between amorphous and crystalline phases. A low electrical pulse will make the PCM device more crystalline (less resistance), this pulse can be repeatedly applied to gradually decrease the device resistance. On the other hand, the material change from the crystalline phase (low resistance) to the amorphouse phase (high resistance) is quite abrupt and requires a high electrical pulse to RESET the device. Therefore, it is possible to records the states as a continuum of values between the two extremes, instead of encoding 0 or 1 like in the digital world.\n",
        "\n",
        "PCM devices have the ability to store synaptic weights in their analog conductance state. When PCM devices are arranged in a crossbar configuration, it allows to perform an analog matrix-vector multiplication in a single time step, exploiting the advantages of multi-level storage capability and Kirchhoff’s circuits laws. The figure below shows how PCM devices are arranged in a crossbar configuration. \n",
        "<center><img src=\"img//pcm-array.png\" style=\"width:30%; height:30%\"/></center> \n",
        "\n",
        "This crossbar configuration is also referred to as an Analog tile. The PCM devices at each crossbard crosspoint are also referred to as Resitive Processing Units or RPU units as shown in the figure below:\n",
        "\n",
        "<center><img src=\"img/pcm_rpu_unit.png\" style=\"width:30%; height:30%\"/></center> \n",
        "\n",
        "Besides PCM, other devices or materials can be used as resistive units or RPUs in the crossbar configuration. Examples include Resistive Random Access Memory (RRAM), Electro chemical Random Access Memory (ECRAM), Magnetic RAM (MRAM), photonics, etc. \n",
        "\n",
        "\n",
        "## Analog AI and Neural Networks\n",
        "\n",
        "In deep learning inference, data propagation through multiple layers of a neural network involves a sequence of matrix multiplications, as each layer can be represented as a matrix of synaptic weights. On an Analog chip, these weights are stored in the conductance states of resistive devices such as PCM. The devices are arranged in crossbar arrays, creating an artificial neural network where all matrix multiplications are performed in-place in an analog manner. This structure allows inference to be performed using little energy with high areal density of synapses. An in-memory computing chip typically consists of multiple crossbar arrays of memory devices that communicate with each other (see figure below). A neural network layer can be implemented on (at least) one crossbar, in which the weights of that layer are stored in the charge or conductance state of the memory devices at the crosspoints.\n",
        "\n",
        "<center><img src=\"img/analog_Dnn.png\" style=\"width:60%; height:60%\"/></center> \n",
        "\n",
        "\n",
        "\n",
        "## IBM Analog Hardware Acceleration Kit (AIHWKIT)\n",
        "\n",
        "The IBM Analog Hardware Acceleration Kit (AIHWKIT) is an open source Python toolkit for exploring and using the capabilities of in-memory computing devices such as PCM in the context of artificial intelligence. \n",
        "The pytorch integration consists of a series of primitives and features that allow using the toolkit within PyTorch. \n",
        "\n",
        "The github repository can be found at: https://github.com/IBM/aihwkit\n",
        "\n",
        "To learn more about Analog AI and the harware befind it, refer to this webpage: https://analog-ai-demo.mybluemix.net/hardware\n",
        "\n",
        "\n",
        "\n",
        "The first thing to do is to install the AIHHKIT and dependencies in your environment. The preferred way to install this package is by using the Python package index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ZMdvhfzHeV",
        "outputId": "00752ae3-8164-4f2d-8e75-8da2fc2dbeb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting aihwkit\n",
            "  Downloading aihwkit-0.4.0-cp37-cp37m-manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from aihwkit) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from aihwkit) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from aihwkit) (0.11.1+cu111)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from aihwkit) (1.4.1)\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.7 kB/s \n",
            "\u001b[?25hCollecting requests<3,>=2.25\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 794 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->aihwkit) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.13.0->aihwkit) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25->aihwkit) (2.0.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25->aihwkit) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25->aihwkit) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25->aihwkit) (1.24.3)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.11.1-cp37-cp37m-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 24.1 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.8 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->aihwkit) (7.1.2)\n",
            "  Downloading torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 560 kB/s \n",
            "\u001b[?25hInstalling collected packages: torch, torchvision, requests, aihwkit\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aihwkit-0.4.0 requests-2.26.0 torch-1.8.1 torchvision-0.9.1\n"
          ]
        }
      ],
      "source": [
        "pip install aihwkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC9XIRtjzHeW"
      },
      "source": [
        "\n",
        "## LeNet5 Neural Network Examples\n",
        "\n",
        "In this notebook we will use the AIHWKIT to train a LeNet5 inspired analog network, as studied in the paper: https://www.frontiersin.org/articles/10.3389/fnins.2017.00538/full\n",
        "\n",
        "<img src=\"https://github.com/kaoutar55/aihwkit/blob/master/notebooks/LeNet5_animation.png?raw=1\" style=\"width:40%; height:40%\"/> \n",
        "\n",
        "The network will be trained using the MNIST dataset, a collection of images representing the digits 0 to 9. The architecture of the LeNet5 network is shown below:\n",
        "\n",
        "<img src=\"img/LeNet.png\" style=\"width:40%; height:40%\"/> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBr6Z5FHzHeX"
      },
      "source": [
        "## Analog layers\n",
        "If the library was installed correctly, you can use the following snippet for creating an analog layer and predicting the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-uJPmFMzHeX",
        "outputId": "d9c997cf-cd3c-47ab-c005-e474d1f98fd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.3294,  0.5176],\n",
              "        [-0.3765,  0.7059]], grad_fn=<AnalogFunctionBackward>)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch import Tensor\n",
        "from aihwkit.nn import AnalogLinear\n",
        "\n",
        "model = AnalogLinear(2, 2)\n",
        "model(Tensor([[0.1, 0.2], [0.3, 0.4]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jehSr26TzHeY"
      },
      "source": [
        "## RPU Configuration\n",
        "Now that the package is installed and running, we can start working on creating the LeNet5 network.\n",
        "\n",
        "AIHWKIT offers different Analog layers that can be used to build a network, including AnalogLinear and AnalogConv2d which will be the main layers used to build the present network. \n",
        "In addition to the standard input that are expected by the PyTorch layers (in_channels, out_channels, etc.) the analog layers also expect a rpu_config input which defines various settings of the RPU tile. \n",
        "\n",
        "Through the rpu_config parameter the user can specify many of the hardware specs such as: device used in the cross-point array, bit used by the ADC/DAC converters, noise values and many other. \n",
        "\n",
        "Additional details on the RPU configuration can be found at https://aihwkit.readthedocs.io/en/latest/using_simulator.html#rpu-configurations\n",
        "\n",
        "For this particular case we will define a RPU which uses RRAM devices in the cross-point array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGNLDA2IzHeZ"
      },
      "outputs": [],
      "source": [
        "def create_rpu_config():\n",
        "\n",
        "    from aihwkit.simulator.presets import ReRamSBPreset\n",
        "\n",
        "    rpu_config=ReRamSBPreset()\n",
        "\n",
        "    return rpu_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soEwkJg3zHeZ"
      },
      "source": [
        "We can now use this rpu_config as input of the network model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBu6VxsJzHea"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Tanh, MaxPool2d, LogSoftmax, Flatten\n",
        "from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n",
        "\n",
        "def create_analog_network(rpu_config):\n",
        "    \n",
        "    channel = [16, 32, 512, 128]\n",
        "    model = AnalogSequential(\n",
        "        AnalogConv2d(in_channels=1, out_channels=channel[0], kernel_size=5, stride=1,\n",
        "                        rpu_config=rpu_config),\n",
        "        Tanh(),\n",
        "        MaxPool2d(kernel_size=2),\n",
        "        AnalogConv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=5, stride=1,\n",
        "                        rpu_config=rpu_config),\n",
        "        Tanh(),\n",
        "        MaxPool2d(kernel_size=2),\n",
        "        Tanh(),\n",
        "        Flatten(),\n",
        "        AnalogLinear(in_features=channel[2], out_features=channel[3], rpu_config=rpu_config),\n",
        "        Tanh(),\n",
        "        AnalogLinear(in_features=channel[3], out_features=10, rpu_config=rpu_config),\n",
        "        LogSoftmax(dim=1)\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW8NXVY1zHeb"
      },
      "source": [
        "## Analog Optimizer \n",
        "\n",
        "We will use the cross entropy to calculate the loss and the Stochastic Gradient Descent (SGD) as optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXd7VR9szHeb"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "\n",
        "from aihwkit.optim import AnalogSGD\n",
        "\n",
        "def create_analog_optimizer(model):\n",
        "    \"\"\"Create the analog-aware optimizer.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): model to be trained\n",
        "\n",
        "    Returns:\n",
        "        Optimizer: created analog optimizer\n",
        "    \"\"\"\n",
        "    \n",
        "    optimizer = AnalogSGD(model.parameters(), lr=0.01) # we will use a learning rate of 0.01 as in the paper\n",
        "    optimizer.regroup_param_groups(model)\n",
        "\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT_5bGZQzHec"
      },
      "source": [
        "## Training the network\n",
        "\n",
        "We can now write the train function which will optimize the network over the MNIST train dataset. The train_step function will take as input the images to train on, the model to train and the criterion and optimizer to train with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHicu1qWzHec",
        "outputId": "e3cbef03-502c-46c0-f165-cae2ea9c6b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running the simulation on:  cpu\n"
          ]
        }
      ],
      "source": [
        "from torch import device\n",
        "from aihwkit.simulator.rpu_base import cuda\n",
        "\n",
        "\n",
        "DEVICE = device('cuda' if cuda.is_compiled() else 'cpu')\n",
        "print('Running the simulation on: ', DEVICE)\n",
        "\n",
        "def train_step(train_data, model, criterion, optimizer):\n",
        "    \"\"\"Train network.\n",
        "\n",
        "    Args:\n",
        "        train_data (DataLoader): Validation set to perform the evaluation\n",
        "        model (nn.Module): Trained model to be evaluated\n",
        "        criterion (nn.CrossEntropyLoss): criterion to compute loss\n",
        "        optimizer (Optimizer): analog model optimizer\n",
        "\n",
        "    Returns:\n",
        "        train_dataset_loss: epoch loss of the train dataset\n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in train_data:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Add training Tensor to the model (input).\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        # Run training (backward propagation).\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize weights.\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "    train_dataset_loss = total_loss / len(train_data.dataset)\n",
        "\n",
        "    return train_dataset_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwMUHVEpzHed"
      },
      "source": [
        "Since training can be quite time consuming it is nice to see the evolution of the training process by testing the model capabilities on a set of images that it has not seen before (test dataset). So we write a test_step function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4-gBviwzHed"
      },
      "outputs": [],
      "source": [
        "def test_step(validation_data, model, criterion):\n",
        "    \"\"\"Test trained network\n",
        "\n",
        "    Args:\n",
        "        validation_data (DataLoader): Validation set to perform the evaluation\n",
        "        model (nn.Module): Trained model to be evaluated\n",
        "        criterion (nn.CrossEntropyLoss): criterion to compute loss\n",
        "\n",
        "    Returns: \n",
        "        test_dataset_loss: epoch loss of the train_dataset\n",
        "        test_dataset_error: error of the test dataset\n",
        "        test_dataset_accuracy: accuracy of the test dataset\n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "    predicted_ok = 0\n",
        "    total_images = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for images, labels in validation_data:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, predicted = torch.max(pred.data, 1)\n",
        "        total_images += labels.size(0)\n",
        "        predicted_ok += (predicted == labels).sum().item()\n",
        "        test_dataset_accuracy = predicted_ok/total_images*100\n",
        "        test_dataset_error = (1-predicted_ok/total_images)*100\n",
        "\n",
        "    test_dataset_loss = total_loss / len(validation_data.dataset)\n",
        "\n",
        "    return test_dataset_loss, test_dataset_error, test_dataset_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JWPjc0BzHee"
      },
      "source": [
        "To reach satisfactory accuracy levels, the train_step will have to be repeated mulitple time so we will implement a loop over a certain number of epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU28l53ozHef"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, criterion, optimizer, train_data, validation_data, epochs=10, print_every=1):\n",
        "    \"\"\"Training loop.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Trained model to be evaluated\n",
        "        criterion (nn.CrossEntropyLoss): criterion to compute loss\n",
        "        optimizer (Optimizer): analog model optimizer\n",
        "        train_data (DataLoader): Validation set to perform the evaluation\n",
        "        validation_data (DataLoader): Validation set to perform the evaluation\n",
        "        epochs (int): global parameter to define epochs number\n",
        "        print_every (int): defines how many times to print training progress\n",
        "\n",
        "    Returns:\n",
        "        nn.Module, Optimizer, Tuple: model, optimizer, and a tuple of\n",
        "            lists of train losses, validation losses, and test error\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    test_error = []\n",
        "\n",
        "    # Train model\n",
        "    for epoch in range(0, epochs):\n",
        "        # Train_step\n",
        "        train_loss = train_step(train_data, model, criterion, optimizer)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        if epoch % print_every == (print_every - 1):\n",
        "            # Validate_step\n",
        "            with torch.no_grad():\n",
        "                valid_loss, error, accuracy = test_step(validation_data, model, criterion)\n",
        "                valid_losses.append(valid_loss)\n",
        "                test_error.append(error)\n",
        "\n",
        "            print(f'Epoch: {epoch} --- '\n",
        "                  f'Train loss: {train_loss:.4f}\\t'\n",
        "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
        "                  f'Test error: {error:.2f}%\\t'\n",
        "                  f'Test accuracy: {accuracy:.2f}%\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odgAhMk2zHef"
      },
      "source": [
        "We will now download the MNIST dataset and prepare the images for the training and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_uhR68azHef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "PATH_DATASET = os.path.join('data', 'DATASET')\n",
        "os.makedirs(PATH_DATASET, exist_ok=True)\n",
        "\n",
        "def load_images():\n",
        "    \"\"\"Load images for train from torchvision datasets.\"\"\"\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train_set = datasets.MNIST(PATH_DATASET, download=True, train=True, transform=transform)\n",
        "    test_set = datasets.MNIST(PATH_DATASET, download=True, train=False, transform=transform)\n",
        "    train_data = torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True)\n",
        "    test_data = torch.utils.data.DataLoader(test_set, batch_size=8, shuffle=False)\n",
        "\n",
        "    return train_data, test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk27OXgCzHeg"
      },
      "source": [
        "Put together all the code above to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "b5OJ0uIbzHeg",
        "outputId": "f7109062-1a9f-424c-bfce-e38ca894d227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 --- Train loss: 2.9887\tValid loss: 2.8677\tTest error: 95.06%\tTest accuracy: 4.94%\t\n",
            "Epoch: 1 --- Train loss: 3.0201\tValid loss: 3.9759\tTest error: 90.22%\tTest accuracy: 9.78%\t\n",
            "Epoch: 2 --- Train loss: 3.0315\tValid loss: 3.1268\tTest error: 89.69%\tTest accuracy: 10.31%\t\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7bef10bcbb2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_analog_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-2b763ee8b999>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, criterion, optimizer, train_data, validation_data, epochs, print_every)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Train_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-86b4510119c0>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(train_data, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Run training (backward propagation).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Optimize weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# _forward_cls is defined by derived class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/aihwkit/nn/functions.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Call the backward function in the tile instance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_indexed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalog_tile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_indexed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalog_tile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/aihwkit/simulator/tiles/base.py\u001b[0m in \u001b[0;36mbackward_indexed\u001b[0;34m(self, d_input)\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'self.image_sizes length is not 3, 5 or 7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_indexed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_indexed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "#load the dataset\n",
        "train_data, test_data = load_images()\n",
        "\n",
        "#create the rpu_config\n",
        "rpu_config = create_rpu_config()\n",
        "\n",
        "#create the model\n",
        "model = create_analog_network(rpu_config).to(DEVICE)\n",
        "\n",
        "#define the analog optimizer\n",
        "optimizer = create_analog_optimizer(model)\n",
        "\n",
        "training_loop(model, criterion, optimizer, train_data, test_data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of LeNet5.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8afadb82c8c635d284d204a78cd7f3b56094702ee8f92f25084bfbbc5b27362b"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('vs': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
